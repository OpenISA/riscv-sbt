MiBench
=======

cc=rvcc=gcc
                locals  abi
1) dijkstra     0.87    0.85
ARM:            0.77    0.80

2) crc32        0.95    0.71
    low %mean (~15%), high %sd
ARM:            1.25    1.48
* reg mode somehow leads to different ordering during opt phase,
  and codegen is able to generate less code in the main loop for
  locals mode.

rijndael: MMX:
3) rijndael-enc 1.27    1.00
AVX:            1.57    1.20
ARM:            1.85    1.40
4) rijndael-dec 1.25    0.99
AVX:            1.43    1.14
ARM:            1.63    1.26

main overheads on x86:
1) reg sync
2) missed LLVM vectorization
   16 bytes -> load -> xor -> store
   RV: unrolled loop that performs multiple loads, xors and stores
   x86: vload, vxor, vstore
   opt/llc: unable to infer that the multiple loads, xors and stores
        could be done with vector instructions
  mention that RISCV vectorization extension should help with this

main overheads on ARM:
- reg sync is also responsible for a big overhead on ARM
- the other main source of overhead for rijndael translated to ARM
  is at encfile()/decfile(). The loop to xor 16 positions of inbuf
  with outbuf is unrolled, using a large amount of registers. When
  translating from RV to ARM, the resulting code ends up doing a lot
  of spills, because it still keeps lots of emulated regs on local vars,
  instead of promoting almost all to host registers.


5) sha          1.73    1.32
MMX:            1.61    1.24
ARM:            1.64    1.48
NO_LOOP_UNROLLING_AT_BYTE_REVERSE:
/*
#pragma GCC push_options
#pragma GCC optimize("no-unroll-loops,no-peel-loops,no-tree-loop-optimize"))
byte_reverse(...) {
...
}
#pragma GCC pop_options
*/
MMX:            1.19    1.09
ARM:            1.59    1.18

NOTE: on ARM, performance doesn't improve much on locals mode because with
loop unrolling disabled the compiler doesn't inline byte_reverse in sha_final
anymore, but instead make calls to byte_reverse, which increases reg syncs.

x86:
1- missed vectorization
2- translated code uses more instructions at sha_transform()
   - x86 seems to move pointer val to register and make memory
     accesses through that, taking full advantage of x86 more
     complex addressing modes, while rv32 code breaks these accesses
     in more parts, by performing calculations that could be done
     directly in mov. Also, rv32 presents more spills of some important
     registers.

x86 && ARM:
- SHA's main source of overhead is somewhat similar to that of Rijndael:
  too many spills when a large number of registers is used.
  In SHA's case, it happens at sha_final(), at the 2 calls to byte_reverse().
  On RV32, byte_reverse() codegen produces a series of loads followed
  by stores, from/to registers directly to an offset at SHA_INFO's data array,
  using all of the 32 RV32 registers as it is possible.
  When the code is translated to x86 or ARM, however, that have a much smaller
  register set, there is a huge number of spills and reloads.
  Native x86/ARM code performs better because codegen limits more the loop
  unrolling, using the number of host registers to limit it.

6) adpcm-enc    0.72    0.70
ARM:            1.14    1.12
7) adpcm-dec    1.25    1.11
ARM:            1.20    1.08

8) stringsearch 2.67    2.85
MMX:            0.94    0.95
ARM:            1.21    1.00
* missed vectorization

9) bf-enc       1.15    1.03
ARM:            1.84    1.33
A) bf-dec       1.14    0.95
ARM:            1.87    1.33
- ARM: biggest overhead is at BF_cfb64_encrypt
  There is a couple of optimizations that the compiler performs when emitting
  code directly to ARM that are lost when translating from RV32 to ARM:
  - combine a lot of loads and reloads (from spilled stuff) into a single
    ldmia instruction
  - preserve content loaded before calling BF_encrypt(), used in n2l() calls,
    to reuse it in the l2n() calls
  - implement l2n() (*(a++)=(b>>c)&0xff) with 4 pairs of lsr+bfi and a single
    32-bit store of the result.
    RV32 has nothing similar to bfi to manipulate bits, so it ends up using
    more instructions and more memory accesses to arrange data correctly.

B) basicmath    0.99    0.99
ARM:            0.98    1.08
x86: * very low %mean (~3%), that also causes big xsd (~0.25)
ARM: * very low %mean (~2%), that also causes big xsd (~0.15)

C) bitcount     1.65    0.80
ARM:            5.25    1.42
* huge reg sync impact
ARM:
 - Even on ABI mode, the register sync is responsible for the 42% overhead on
   ARM. On native mode, the compiler is smart enough to use very few registers
   in the main loop and, above that, load/reload only the registers that: 1-
   it will need inside the loop; 2- are used to pass arguments to the indirect
   function called.
   When translated, all registers that had any modification and may transfer
   data to/from the called function are synchronized. ABI mode reduces this
   number drasticaly, but still sync many more registers than native compile
   mode, thus the big overhead.
   However, this is not due to hard to emulate aspects of RISC-V, as this
   overhead could be eliminated by improvements in the SBT, by making it
   move reg syncs of unchanged registers inside the loop to the outside.
   (LIRSM - loop invariant reg sync motion :) )

D) fft-std      1.17    1.13
ARM:            1.04    0.98
x86: * low %mean (~5%)
ARM: * low %mean (~7%)
E) fft-inv      1.16    1.12
ARM:            1.05    0.99
x86: * low %mean (~8%)
ARM: * low %mean (~13%)

F) patricia     0.90    0.99
ARM:            1.40    1.19
x86: * low %mean (~4%), high xsd (~0.3)
ARM: * low %mean (~3%), high xsd (~0.2)

G) susan-smooth 1.13    1.03
MMX:            0.76    0.69
ARM:            1.23    1.15
H) susan-edges  1.26    1.23
MMX:            1.13    1.09
ARM:            1.25    1.19
I) susan-corner 1.34    1.27
MMX:            0.94    0.95
ARM:            1.15    1.17
* missed vectorization

--> ARM <--

J) lame         1.44    1.12
ARM:            1.95    1.46

ARM:
On ARM, the biggest overhead is at window_subband.constprop.28() function.
However, LAME is quite big, and other parts of it also present significant
overhead. But, for timing constraints, we've analyzed only window_subband().


First, let's consider the experimental data collected:

Measurement data (RV32-ARM = rv32-arm-lame-abi):
            time% at lame bin   time% at window_subband     slowdown
ARM:        91.68%              13.71%
RV32-ARM:   93.70%              15.86%                      1.46x

Note that although % of time is not much higher on RV32-ARM, this % refers to
a much bigger total execution time.

Perf samples:   pre-loop    loop1   loop2   total
ARM:            10          241     288     539
RV32-ARM:       17          421     863     1301


Now, let's look at how code was generated.

The hot spot source code has the following format:
for 15..0:
    for 14..0:
        s0 += *wp++ * *in++;
        s1 += *wp++ * *in++;

Where,
wp = mm (uninitialized static data)
in = func param = win (uninitialized static data)
s0, s1 = local vars

CodeGen:
- In all cases the inner loop was completely unrolled.

ARM (each iteration):
4x vldr
2x vmla.f64
total = 6 instructions

RV32:
4x fld offs(a4/a5)
2x fmadd
total = 6 instructions

RV32-ARM:
2x
1- movw r0, %lo(addr)
2- movt r0, %hi(addr)
3- add r0, ip
4- vldr dx, [r0]
5- add.w rx, r2, #imm
6- vldr dx, [rx]
7- vmla.f64
total = 14 instructions

We can see that, for both ARM and RV32, the generated code is optimal. It
performs only the minimum required operations: 4 loads and 2 fused multiply
adds of loaded values.
When translating from RV32 to ARM however, the number of instructions more
than double. We can divide the inneficiencies in two.

1)
On RV32-ARM, instructions 1 to 3 are used to load an immediate address, that
is then used by instruction 4 to load a value from there. On RV32, these are
a4 relative addresses. At the inner loop, a4 has a known value and that's why
references using it are translated to immediate addresses on ARM.
The main problem here seems to be that, when translating the
optimized/unrolled RV32 loop, the optimizer/code generator fails to infer that
all these addresses are relative to a common base, with an offset added only.
Add to that the fact that, on ARM, an immediate address in Position
Independent Code (PIC) requires 3 instructions to be loaded into a register:
load the lower part (movw), higher part (movt) and add the position
independent base (ip).
On native ARM codegen, loads are performed using a base register plus an
offset. Going even further, the compiler adds an offset to the base register,
to make it possible to use the limited immediate offset field of vldr
instruction (-1020..1020), saving a register and an add instruction on each
load.
Thus, for native ARM, the compiler is able to maintain the whole view of the
hottest nested loop and generate the most efficient code for it. That's not
the case when translating the already unrolled and optimized RV32 loop code.
Performance could possibly be increase if generating ARM code that need not be
position independent or making RV32 code be position independent, so both
optimizer would match on this feature. Our ARM toolchain however requires PIC
and there is no time available to change the experiments to use RV32 PIC.

2)
On RV32, a5 value inside the main basic block varies (it's assigned from a
phi node). That's why addresses derived from it can't be converted into
immediate values, as in case 1. When generating ARM code, a5 based loads
results in 2 instructions: 5 (r2+offset) and 6 (vldr). In this case, the shift
base register value to make offsets fit in vldr immediate field optimization
was missed, as in 1.
