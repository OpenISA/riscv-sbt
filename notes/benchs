MiBench
=======

cc=clang
rvcc=gcc
                locals  abi
1) dijkstra     0.87    0.85
ARM:            0.77    0.80

2) crc32        0.95    0.71
    low %mean (~15%), high %sd
ARM:            1.25    1.48
* reg mode somehow leads to different ordering during opt phase,
  and codegen is able to generate less code in the main loop for
  locals mode.

rijndael: MMX:
3) rijndael-enc 1.27    1.00
AVX:            1.57    1.20
ARM:            1.85    1.40
4) rijndael-dec 1.25    0.99
AVX:            1.43    1.14
ARM:            1.63    1.26

main overheads on x86:
1) reg sync
2) missed LLVM vectorization
   16 bytes -> load -> xor -> store
   RV: unrolled loop that performs multiple loads, xors and stores
   x86: vload, vxor, vstore
   opt/llc: unable to infer that the multiple loads, xors and stores
        could be done with vector instructions
  mention that RISCV vectorization extension should help with this

main overheads on ARM:
- reg sync is also responsible for a big overhead on ARM
- the other main source of overhead for rijndael translated to ARM
  is at encfile()/decfile(). The loop to xor 16 positions of inbuf
  with outbuf is unrolled, using a large amount of registers. When
  translating from RV to ARM, the resulting code ends up doing a lot
  of spills, because it still keeps lots of emulated regs on local vars,
  instead of promoting almost all to host registers.

--> ARM <--

5) sha          1.73    1.32
MMX:            1.61    1.24
ARM:            1.64    1.48

x86:
1- missed vectorization
2- translated code uses more instructions at sha_transform()
   - x86 seems to move pointer val to register and make memory
     accesses through that, taking full advantage of x86 more
     complex addressing modes, while rv32 code breaks these accesses
     in more parts, by performing calculations that could be done
     directly in mov. Also, rv32 presents more spills of some important
     registers.

ARM:
- SHA's main source of overhead is similar to that of Rijndael: too many
  spills when a large number of registers is used.
  In SHA's case, it happens at sha_final(), at the 2 calls to byte_reverse().
  On RV and ARM, byte_reverse() codegen produces a series of loads followed
  by stores, from/to registers directly to an offset on the stack.
  *
  On RV32-ARM, at byte_reverse(), there is a huge number of spills first,
  followed by loads and stores with a considerable number of spills/reloads
  between them.
  The large number of spills corresponds mainly to pointers to guest stack.
  Since ARM
  On the translated code, we have 2 stacks: guest and native.
  ***
  TODO: load offs(reg)/store offs(reg) is not being translated efficiently,
        to only one host instruction.

6) adpcm-enc    0.72    0.70
7) adpcm-dec    1.25    1.11

8) stringsearch 2.67    2.85
MMX:            0.94    0.95
* missed vectorization

9) bf-enc       1.15    1.03
A) bf-dec       1.14    0.95

B) basicmath    0.99    0.99
* very low %mean (~3%), that also causes big xsd (~0.25)

C) bitcount     1.75    0.81
* huge reg sync impact

D) fft-std      1.17    1.13
* low %mean (~5%)
E) fft-inv      1.16    1.12
* low %mean (~8%)

F) patricia     0.90    0.99
* low %mean (~4%), high xsd (~0.3)

G) susan-smooth 1.13    1.03
MMX:            0.76    0.69
H) susan-edges  1.26    1.23
MMX:            1.13    1.09
I) susan-corner 1.34    1.27
MMX:            0.94    0.95
* missed vectorization

J) lame         1.44    1.12
